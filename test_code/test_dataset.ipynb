{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "\n",
    "\n",
    "class BatchSchedulerSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"\n",
    "    iterate over tasks and provide a random batch per task in each mini-batch\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size, rank, gpus, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.number_of_datasets = len(dataset.datasets)\n",
    "        self.largest_dataset_size = max([len(cur_dataset) for cur_dataset in dataset.datasets])\n",
    "\n",
    "        self.number_selected_samples = int(self.batch_size * math.ceil(self.largest_dataset_size / self.batch_size) * len(self.dataset.datasets) / gpus)\n",
    "        self.number_of_total_size = self.number_selected_samples*gpus\n",
    "        print('number_selected_samples', self.number_selected_samples)\n",
    "        print('total sample epoch', self.number_of_datasets * self.largest_dataset_size)\n",
    "        self.gpus = gpus\n",
    "        self.rank = rank\n",
    "        self.shuffle = shuffle\n",
    "        self.epoch = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.number_selected_samples\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            # deterministically shuffle based on epoch\n",
    "            g = torch.Generator()\n",
    "            g.manual_seed(self.epoch)\n",
    "            # indices = torch.randperm(len(self.dataset), generator=g)\n",
    "        else:\n",
    "            g = torch.Generator()\n",
    "            g.manual_seed(0)\n",
    "\n",
    "        samplers_list = []\n",
    "        sampler_iterators = []\n",
    "        for dataset_idx in range(self.number_of_datasets):\n",
    "            cur_dataset = self.dataset.datasets[dataset_idx]\n",
    "            sampler = RandomSampler(cur_dataset, generator=g)\n",
    "            samplers_list.append(sampler)\n",
    "            cur_sampler_iterator = iter(list(sampler.__iter__())[self.rank:self.number_selected_samples:self.gpus])\n",
    "            sampler_iterators.append(cur_sampler_iterator)\n",
    "\n",
    "        push_index_val = [0] + self.dataset.cumulative_sizes[:-1]\n",
    "        step = self.batch_size * self.number_of_datasets\n",
    "        samples_to_grab = self.batch_size\n",
    "        \n",
    "        # for this case we want to get all samples in dataset, this force us to resample from the smaller datasets\n",
    "        epoch_samples = self.largest_dataset_size * self.number_of_datasets\n",
    "\n",
    "        final_samples_list = []  # this is a list of indexes from the combined dataset\n",
    "        for _ in range(0, epoch_samples, step):\n",
    "            for i in range(self.number_of_datasets):\n",
    "                cur_batch_sampler = sampler_iterators[i]\n",
    "                cur_samples = []\n",
    "                for _ in range(samples_to_grab):\n",
    "                    try:\n",
    "                        cur_sample_org = cur_batch_sampler.__next__()\n",
    "                        cur_sample = cur_sample_org + push_index_val[i]\n",
    "                        cur_samples.append(cur_sample)\n",
    "                    except StopIteration:\n",
    "                        # got to the end of iterator - restart the iterator and continue to get samples\n",
    "                        # until reaching \"epoch_samples\"\n",
    "                        sampler_iterators[i] = iter(list(samplers_list[i].__iter__())[self.rank:self.number_of_total_size:self.gpus])\n",
    "                        cur_batch_sampler = sampler_iterators[i]\n",
    "                        cur_sample_org = cur_batch_sampler.__next__()\n",
    "                        cur_sample = cur_sample_org + push_index_val[i]\n",
    "                        cur_samples.append(cur_sample)\n",
    "                final_samples_list.extend(cur_samples)\n",
    "        \n",
    "        return iter(final_samples_list[:self.number_selected_samples])\n",
    "    \n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "\n",
    "from typing import Iterable\n",
    "from torch.utils.data import TensorDataset, ConcatDataset, DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import bisect\n",
    "\n",
    "class ConCatDatasetWithIndex(ConcatDataset):\n",
    "    def __init__(self, datasets: Iterable[Dataset]) -> None:\n",
    "        super().__init__(datasets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0:\n",
    "            if -idx > len(self):\n",
    "                raise ValueError(\"absolute value of index should not exceed dataset length\")\n",
    "            idx = len(self) + idx\n",
    "        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n",
    "        if dataset_idx == 0:\n",
    "            sample_idx = idx\n",
    "        else:\n",
    "            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n",
    "        return dataset_idx, self.datasets[dataset_idx][sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, size=64):\n",
    "        super().__init__()\n",
    "        self.size=size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 15\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.randn((self.size // 8, 3, self.size, self.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = TestDataset(64)\n",
    "d2 = TestDataset(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "Concat_dataset = ConCatDatasetWithIndex([d1, d2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_selected_samples 30\n",
      "total sample epoch 30\n"
     ]
    }
   ],
   "source": [
    "sampler = BatchSchedulerSampler(dataset=Concat_dataset, batch_size=1, rank=0, gpus=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset=Concat_dataset, batch_size=1, shuffle=False, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor(0)\n",
      "torch.Size([1, 8, 3, 64, 64])\n",
      "1\n",
      "tensor(1)\n",
      "torch.Size([1, 16, 3, 128, 128])\n",
      "2\n",
      "tensor(0)\n",
      "torch.Size([1, 8, 3, 64, 64])\n",
      "3\n",
      "tensor(1)\n",
      "torch.Size([1, 16, 3, 128, 128])\n",
      "4\n",
      "tensor(0)\n",
      "torch.Size([1, 8, 3, 64, 64])\n",
      "5\n",
      "tensor(1)\n",
      "torch.Size([1, 16, 3, 128, 128])\n",
      "6\n",
      "tensor(0)\n",
      "torch.Size([1, 8, 3, 64, 64])\n",
      "7\n",
      "tensor(1)\n",
      "torch.Size([1, 16, 3, 128, 128])\n",
      "8\n",
      "tensor(0)\n",
      "torch.Size([1, 8, 3, 64, 64])\n",
      "9\n",
      "tensor(1)\n",
      "torch.Size([1, 16, 3, 128, 128])\n",
      "10\n",
      "tensor(0)\n",
      "torch.Size([1, 8, 3, 64, 64])\n",
      "11\n",
      "tensor(1)\n",
      "torch.Size([1, 16, 3, 128, 128])\n",
      "12\n",
      "tensor(0)\n",
      "torch.Size([1, 8, 3, 64, 64])\n",
      "13\n",
      "tensor(1)\n",
      "torch.Size([1, 16, 3, 128, 128])\n",
      "14\n",
      "tensor(0)\n",
      "torch.Size([1, 8, 3, 64, 64])\n",
      "15\n",
      "tensor(1)\n",
      "torch.Size([1, 16, 3, 128, 128])\n",
      "16\n",
      "tensor(0)\n",
      "torch.Size([1, 8, 3, 64, 64])\n",
      "17\n",
      "tensor(1)\n",
      "torch.Size([1, 16, 3, 128, 128])\n",
      "18\n",
      "tensor(0)\n",
      "torch.Size([1, 8, 3, 64, 64])\n",
      "19\n",
      "tensor(1)\n",
      "torch.Size([1, 16, 3, 128, 128])\n",
      "20\n",
      "tensor(0)\n",
      "torch.Size([1, 8, 3, 64, 64])\n",
      "21\n",
      "tensor(1)\n",
      "torch.Size([1, 16, 3, 128, 128])\n",
      "22\n",
      "tensor(0)\n",
      "torch.Size([1, 8, 3, 64, 64])\n",
      "23\n",
      "tensor(1)\n",
      "torch.Size([1, 16, 3, 128, 128])\n",
      "24\n",
      "tensor(0)\n",
      "torch.Size([1, 8, 3, 64, 64])\n",
      "25\n",
      "tensor(1)\n",
      "torch.Size([1, 16, 3, 128, 128])\n",
      "26\n",
      "tensor(0)\n",
      "torch.Size([1, 8, 3, 64, 64])\n",
      "27\n",
      "tensor(1)\n",
      "torch.Size([1, 16, 3, 128, 128])\n",
      "28\n",
      "tensor(0)\n",
      "torch.Size([1, 8, 3, 64, 64])\n",
      "29\n",
      "tensor(1)\n",
      "torch.Size([1, 16, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "for idx, i in enumerate(dataloader):\n",
    "    idy, batch = i\n",
    "    print(idx)\n",
    "    print(idy[0].item())\n",
    "    print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openstl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
